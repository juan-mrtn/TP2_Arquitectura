import requests
import readline
import json

last_query = ""

def send_to_llama(user_query):
    """
    Env칤a el prompt al modelo llama3 usando la API local de Ollama y devuelve la respuesta.
    """
    try:
        response = requests.post(
            "http://localhost:11434/api/chat",
            json={
                "model": "llama3",
                "messages": [
                    {"role": "system", "content": "Sos un asistente 칰til."},
                    {"role": "user", "content": user_query}
                ]
            },
            stream=True
        )

        full_response = ""
        if response.status_code == 200:
            print("Conexi칩n exitosa con LLaMA.")
        for line in response.iter_lines():
            if line:
                parsed = json.loads(line.decode("utf-8"))
                if "message" in parsed and "content" in parsed["message"]:
                    full_response += parsed["message"]["content"]

        return full_response or "[No se recibi칩 respuesta v치lida]"
    except Exception as e:
        return f"[ERROR al invocar a LLaMA]: {e}"

def main():
    """
    Funci칩n principal que ejecuta el ciclo de conversaci칩n con el modelo.
    """
    global last_query
    print("Escrib칤 tu consulta para LLaMA. Presion치 Ctrl+C para salir.")
    try:
        while True:
            try:
                
                user_input = input("You: ")
                if user_input.strip() == "":
                    print("[Advertencia]: La consulta est치 vac칤a.")
                    continue

                last_query = user_input
                readline.add_history(user_input)

                print("Enviando a LLaMA...")
                response = send_to_llama(user_input)
                print(f"LLaMa: {response}\n")
            except Exception as e:
                print(f"[ERROR en el procesamiento de la consulta]: {e}")
    except KeyboardInterrupt:
        print("\n游녦 Programa finalizado por el usuario.")

if __name__ == "__main__":
    main()
